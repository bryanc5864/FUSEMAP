#!/usr/bin/env python3
"""
Complete TileFormer Training Pipeline

Integrates all components:
1. Comprehensive 50k sequence corpus generation
2. TLEaP-ABPS electrostatic potential labeling
3. TileFormer model training (exact architecture)
4. Comprehensive evaluation with all metrics

This script implements the full pipeline with minimal files and maximum functionality.
"""

import os
import sys
import time
import logging
import argparse
from pathlib import Path
from typing import Dict, Any, Tuple, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# Import TileFormer components
from data.comprehensive_corpus_generator import ComprehensiveCorpusGenerator
from electrostatics.compressed_abps_runner import CompressedABPSRunner
from electrostatics.tleap_abps_processor import ABPSConfig
from models.tileformer_exact import TileFormer, TileFormerConfig, count_parameters
from evaluation.comprehensive_metrics import ComprehensiveEvaluator

# Enhanced logging configuration with file storage
def setup_logging(logs_dir: str = 'logs'):
    """Setup comprehensive logging to files and console."""
    import datetime
    
    # Create logs directory
    logs_path = Path(logs_dir)
    logs_path.mkdir(parents=True, exist_ok=True)
    
    # Generate timestamp for log files
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Clear any existing handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # Create formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    simple_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Main training log
    main_log = logs_path / f'tileformer_training_{timestamp}.log'
    main_handler = logging.FileHandler(main_log)
    main_handler.setFormatter(detailed_formatter)
    main_handler.setLevel(logging.INFO)
    
    # Debug log (more verbose)
    debug_log = logs_path / f'tileformer_debug_{timestamp}.log'
    debug_handler = logging.FileHandler(debug_log)
    debug_handler.setFormatter(detailed_formatter)
    debug_handler.setLevel(logging.DEBUG)
    
    # ABPS-specific log
    abps_log = logs_path / f'abps_processing_{timestamp}.log'
    abps_handler = logging.FileHandler(abps_log)
    abps_handler.setFormatter(detailed_formatter)
    abps_handler.setLevel(logging.INFO)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(simple_formatter)
    console_handler.setLevel(logging.INFO)
    
    # Setup root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(main_handler)
    root_logger.addHandler(debug_handler)
    root_logger.addHandler(console_handler)
    
    # Setup ABPS logger
    abps_logger = logging.getLogger('electrostatics')
    abps_logger.addHandler(abps_handler)
    abps_logger.propagate = True  # Also send to root handlers
    
    print(f"üìù Logging setup complete:")
    print(f"   Main log: {main_log}")
    print(f"   Debug log: {debug_log}")
    print(f"   ABPS log: {abps_log}")
    
    return str(main_log), str(debug_log), str(abps_log)

# Initialize logging (will be called again in trainer initialization)
setup_logging()
logger = logging.getLogger(__name__)

class DNADataset(Dataset):
    """Dataset for DNA sequences with electrostatic potentials."""
    
    def __init__(
        self, 
        sequences: list, 
        potentials: np.ndarray,
        use_reverse_complement: bool = True,
        use_shift_augmentation: bool = True,
        max_shift: int = 2
    ):
        """
        Initialize DNA dataset.
        
        Args:
            sequences: List of DNA sequences (20bp)
            potentials: Electrostatic potentials (œà values)
            use_reverse_complement: Apply RC augmentation
            use_shift_augmentation: Apply shift augmentation
            max_shift: Maximum shift for augmentation
        """
        self.sequences = sequences
        self.potentials = potentials
        self.use_reverse_complement = use_reverse_complement
        self.use_shift_augmentation = use_shift_augmentation
        self.max_shift = max_shift
        
        # Base mapping for one-hot encoding
        self.base_to_idx = {'A': 0, 'T': 1, 'G': 2, 'C': 3}
        self.complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}
    
    def __len__(self):
        return len(self.sequences)
    
    def _reverse_complement(self, sequence: str) -> str:
        """Generate reverse complement of DNA sequence."""
        return ''.join(self.complement[base] for base in reversed(sequence))
    
    def _shift_sequence(self, sequence: str, shift: int) -> str:
        """Apply shift augmentation (circular shift)."""
        if shift == 0:
            return sequence
        return sequence[shift:] + sequence[:shift]
    
    def _one_hot_encode(self, sequence: str) -> torch.Tensor:
        """Convert DNA sequence to one-hot encoding."""
        # Ensure sequence is exactly 20bp
        sequence = sequence[:20]
        if len(sequence) < 20:
            sequence += 'A' * (20 - len(sequence))  # Pad with A
        
        # One-hot encode
        one_hot = torch.zeros(4, 20)
        for i, base in enumerate(sequence):
            if base in self.base_to_idx:
                one_hot[self.base_to_idx[base], i] = 1.0
            else:
                # Handle ambiguous bases - use A
                one_hot[0, i] = 1.0
        
        return one_hot
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get one sample from dataset."""
        sequence = self.sequences[idx]
        potential = self.potentials[idx]
        
        # Apply augmentations randomly during training
        if self.use_reverse_complement and np.random.random() < 0.5:
            sequence = self._reverse_complement(sequence)
        
        if self.use_shift_augmentation and self.max_shift > 0:
            shift = np.random.randint(-self.max_shift, self.max_shift + 1)
            sequence = self._shift_sequence(sequence, shift)
        
        # One-hot encode
        x = self._one_hot_encode(sequence)
        y = torch.tensor(potential, dtype=torch.float32)
        
        return x, y

class TileFormerTrainer:
    """Complete TileFormer training pipeline."""
    
    def __init__(self, config_path: Optional[str] = None, gpu_id: int = 1):
        """Initialize trainer with configuration."""
        self.config = self._load_config(config_path)
        
        # Force use of specific GPU
        if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
            self.device = torch.device(f'cuda:{gpu_id}')
            torch.cuda.set_device(self.device)
        else:
            available_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
            raise RuntimeError(f"‚ùå GPU {gpu_id} not available! Available GPUs: {available_gpus}")
        
        # Setup enhanced logging with proper directories
        logs_dir = self.config.get('logs_dir', 'logs')
        self.log_files = setup_logging(logs_dir)
        logger.info(f"üìù Enhanced logging initialized in {logs_dir}/")
        
        # Initialize components
        self.corpus_generator = None
        self.abps_runner = None
        self.model = None
        self.evaluator = ComprehensiveEvaluator(
            save_plots=True,
            plot_dir=self.config.get('plots_dir', 'plots')
        )
        
        # Device and memory management
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
            logger.info(f"üöÄ TileFormer Trainer initialized")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   GPU Memory: {gpu_memory:.1f} GB")
        else:
            logger.info(f"üöÄ TileFormer Trainer initialized")
            logger.info(f"   Device: {self.device} (CPU)")
        
        logger.info(f"   Output dir: {self.config.get('output_dir', 'outputs')}")
        
        # Set memory management for better training stability
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
            torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster training
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load configuration from file or use defaults."""
        default_config = {
            # Data paths
            'encode4_path': '/home/bcheng/mainproject/data/human_genome_train.tsv',
            'output_dir': 'outputs',
            'data_dir': 'data',
            'plots_dir': 'plots',
            'logs_dir': 'logs',
            
            # Corpus generation
            'corpus_size': 50000,
            'corpus_seed': 42,
            
            # ABPS settings
            'abps_method': 'tleap',  # 'tleap', 'placeholder'
            'abps_n_processes': 16,
            'abps_batch_size': 100,
            'abps_cleanup': True,
            
            # Model architecture
            'stem_out_channels': 64,
            'd_model': 192,
            'n_heads': 4,
            'n_layers': 2,
            'ffn_dim': 256,
            'dropout': 0.1,
            'mlp_hidden_dims': [128, 64],
            'mlp_dropout': 0.1,
            
            # Training
            'learning_rate': 3e-4,
            'weight_decay': 1e-5,
            'batch_size': 256,
            'max_epochs': 100,
            'train_split': 0.8,
            'val_split': 0.1,
            'early_stopping_patience': 10,
            'grad_clip': 1.0,
            
            # Data augmentation
            'use_reverse_complement': True,
            'use_shift_augmentation': True,
            'max_shift': 2,
            
            # Evaluation
            'test_batch_size': 512,
            'save_predictions': True
        }
        
        if config_path and os.path.exists(config_path):
            import yaml
            with open(config_path, 'r') as f:
                user_config = yaml.safe_load(f)
            default_config.update(user_config)
        
        return default_config
    
    def step1_generate_corpus(self) -> str:
        """Step 1: Generate comprehensive sequence corpus."""
        logger.info("=" * 60)
        logger.info("üß¨ STEP 1: Generating comprehensive sequence corpus")
        logger.info("=" * 60)
        
        start_time = time.time()
        corpus_size = self.config['corpus_size']
        
        # Dynamic corpus filename based on size
        if corpus_size == 50000:
            corpus_filename = 'corpus_50k_complete.tsv'
        else:
            corpus_filename = f'corpus_{corpus_size}_complete.tsv'
        
        # Check if corpus already exists
        corpus_path = Path(self.config['data_dir']) / corpus_filename
        if corpus_path.exists():
            logger.info(f"‚úÖ Corpus already exists: {corpus_path}")
            return str(corpus_path)
        
        logger.info(f"üìä Generating {corpus_size:,} sequences")
        
        if corpus_size == 50000:
            # Use full corpus generator for 50k sequences
            self.corpus_generator = ComprehensiveCorpusGenerator(
                encode4_path=self.config['encode4_path'],
                output_dir=self.config['data_dir'],
                seed=self.config['corpus_seed']
            )
            corpus_df = self.corpus_generator.generate_complete_corpus()
        else:
            # Generate smaller corpus by sampling from existing or creating minimal corpus
            full_corpus_path = Path(self.config['data_dir']) / 'corpus_50k_complete.tsv'
            
            if full_corpus_path.exists():
                # Sample from existing full corpus
                logger.info(f"üìÇ Sampling {corpus_size} sequences from existing full corpus")
                import pandas as pd
                full_df = pd.read_csv(full_corpus_path, sep='\t')
                corpus_df = full_df.sample(n=min(corpus_size, len(full_df)), random_state=self.config['corpus_seed']).reset_index(drop=True)
            else:
                # Create minimal corpus for testing
                logger.info(f"üß™ Creating minimal test corpus with {corpus_size} sequences")
                import pandas as pd
                import numpy as np
                
                sequences = []
                seq_ids = []
                for i in range(corpus_size):
                    # Generate simple test sequences
                    bases = ['A', 'T', 'C', 'G']
                    seq = ''.join(np.random.choice(bases, 20))
                    sequences.append(seq)
                    seq_ids.append(f'test_seq_{i:04d}')
                
                corpus_df = pd.DataFrame({
                    'seq_id': seq_ids,
                    'sequence': sequences,
                    'category': ['test'] * corpus_size,
                    'seq_type': ['random'] * corpus_size,
                    'group_id': ['test'] * corpus_size,
                    'sequence_length': [20] * corpus_size,
                    'gc_content': [50.0] * corpus_size,
                    'cpg_density': [0.1] * corpus_size
                })
            
            # Save corpus
            corpus_path.parent.mkdir(parents=True, exist_ok=True)
            corpus_df.to_csv(corpus_path, sep='\t', index=False)
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Corpus generation completed in {elapsed:.1f}s")
        logger.info(f"üìä Generated {len(corpus_df):,} unique sequences")
        
        return str(corpus_path)
    
    def step2_compute_electrostatics(self, corpus_path: str) -> str:
        """Step 2: Compute electrostatic potentials using TLEaP-ABPS."""
        logger.info("=" * 60)
        logger.info("‚ö° STEP 2: Computing electrostatic potentials")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Dynamic output filename based on corpus size
        corpus_size = self.config['corpus_size']
        if corpus_size == 50000:
            output_filename = 'corpus_50k_with_abps.tsv'
        else:
            output_filename = f'corpus_{corpus_size}_with_abps.tsv'
        
        # Check if potentials already computed
        output_path = Path(self.config['data_dir']) / output_filename
        if output_path.exists():
            logger.info(f"‚úÖ Electrostatic potentials already computed: {output_path}")
            return str(output_path)
        
        # Load corpus
        corpus_df = pd.read_csv(corpus_path, sep='\t')
        
        method = self.config['abps_method']
        
        if method == 'tleap':
            logger.info("üß¨ Using TLEaP-ABPS processor with compressed storage (full physics)")
            
            # Create ABPS configuration
            abps_config = ABPSConfig()
            
            try:
                # Initialize compressed ABPS runner
                self.compressed_abps_runner = CompressedABPSRunner(
                    storage_dir="/shared/data06/TileFormer_ABPS",
                    config=abps_config,
                    n_processes=self.config['abps_n_processes'],
                    batch_size=1000,  # Process 1000 sequences before compression
                    compression_level=22  # Ultra-aggressive compression
                )
                
                # Test with a single sequence first to catch segfaults
                logger.info("üß™ Testing TLEaP with single sequence...")
                test_result = self.compressed_abps_runner.process_corpus(
                    ["ATCGATCGATCGATCGATCG"], ["test_seq"]
                )
                
                if test_result.get("test_seq") is None:
                    logger.warning("‚ö†Ô∏è TLEaP test failed (likely segfault) - using placeholders")
                    method = 'placeholder'
                else:
                    logger.info(f"‚úÖ TLEaP test successful: œà = {test_result['test_seq']:.3f} kT/e")
                    # Process full corpus with compressed storage
                    logger.info(f"üß¨ Processing {len(corpus_df)} sequences with compressed TLEaP-ABPS...")
                    logger.info(f"‚öôÔ∏è Using {self.config['abps_n_processes']} parallel processes")
                    logger.info(f"üóúÔ∏è Storage: /shared/data06/TileFormer_ABPS with zstd compression")
                    logger.info(f"üì¶ Batch size: 1000 sequences per compression cycle")
                    
                    # Extract sequences and IDs
                    sequences = corpus_df['sequence'].tolist()
                    seq_ids = corpus_df['seq_id'].tolist()
                    
                    # Progress tracking
                    total_processed = 0
                    extreme_values = []
                    
                    def progress_callback(current, total):
                        nonlocal total_processed
                        total_processed = current
                        
                    # Process with compressed storage
                    results = self.compressed_abps_runner.process_corpus(
                        sequences=sequences,
                        seq_ids=seq_ids,
                        progress_callback=progress_callback
                    )
                    
                    # Merge results with corpus
                    output_path_temp = str(output_path).replace('.tsv', '_temp.tsv')
                    final_output = self.compressed_abps_runner.merge_with_corpus(
                        corpus_path=corpus_path,
                        output_path=output_path_temp
                    )
                    
                    # Load merged corpus for statistics
                    corpus_with_psi = pd.read_csv(final_output, sep='\t')
                    success_count = corpus_with_psi['electrostatic_potential'].notna().sum()
                    
                    # Final statistics
                    valid_psi = corpus_with_psi['electrostatic_potential'].dropna()
                    if len(valid_psi) > 0:
                        logger.info(f"‚úÖ Compressed TLEaP-ABPS completed: {success_count}/{len(corpus_df)} successful")
                        logger.info(f"üìä Final œà statistics: {valid_psi.mean():.2f}¬±{valid_psi.std():.2f} kT/e")
                        logger.info(f"üìà œà range: [{valid_psi.min():.2f}, {valid_psi.max():.2f}] kT/e")
                        
                        # Check for extreme values
                        extreme_mask = valid_psi.abs() > 200
                        if extreme_mask.any():
                            extreme_count = extreme_mask.sum()
                            logger.warning(f"‚ö†Ô∏è Found {extreme_count} extreme œà values (|œà| > 200 kT/e)")
                        
                        # Get compression summary
                        compression_summary = self.compressed_abps_runner.get_compression_summary()
                        logger.info(f"üóúÔ∏è Compression summary:")
                        logger.info(f"   Batches processed: {compression_summary['batches_processed']}")
                        logger.info(f"   Total compressed size: {compression_summary['total_compressed_size_mb']:.1f} MB")
                        logger.info(f"   Archives created: {len(compression_summary['compressed_archives'])}")
                        
                        # Rename temp file to final output
                        Path(final_output).rename(output_path)
                        
                    else:
                        logger.error("‚ùå No successful ABPS calculations!")
                        method = 'placeholder'
                    
                    # If too many failures, fall back to placeholders
                    if success_count < len(corpus_df) * 0.5:  # Less than 50% success
                        logger.warning("‚ö†Ô∏è TLEaP success rate too low - using placeholders")
                        method = 'placeholder'
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Compressed TLEaP-ABPS encountered issues: {e}")
                logger.info("üîß Falling back to placeholder values")
                method = 'placeholder'
        
        if method == 'placeholder':
            logger.info("üîß Using placeholder electrostatic potentials (fast)")
            logger.info("üßÆ Generating physics-based approximations...")
            
            # Generate physics-based placeholders with progress monitoring
            np.random.seed(self.config['corpus_seed'])
            
            # Base electrostatic potential on sequence properties
            gc_content = corpus_df['gc_content'].values
            cpg_density = corpus_df.get('cpg_density', pd.Series(0.1, index=corpus_df.index)).values
            
            # Enhanced physics-based model with progress bar
            corpus_with_psi = corpus_df.copy()
            base_potentials = []
            
            with tqdm(total=len(corpus_df), desc="üßÆ Computing placeholders", unit="seq") as pbar:
                for i, (gc, cpg) in enumerate(zip(gc_content, cpg_density)):
                    # Physics-based approximation: œà ‚àù -GC% (more negative for GC-rich)
                    base_psi = -0.5 * gc - 0.2 * cpg  # Scaled to reasonable range
                    
                    # Add sequence-specific variations
                    seq = corpus_df.iloc[i]['sequence']
                    
                    # Additional physics factors
                    at_content = (seq.count('A') + seq.count('T')) / len(seq) * 100
                    purine_content = (seq.count('A') + seq.count('G')) / len(seq) * 100
                    
                    # Fine-tune based on sequence composition
                    base_psi += 0.1 * (50 - at_content) / 50  # AT-rich slightly less negative
                    base_psi += 0.05 * (purine_content - 50) / 50  # Purine effect
                    
                    base_potentials.append(base_psi)
                    
                    if (i + 1) % 1000 == 0:
                        pbar.set_postfix({'Avg_œà': f'{np.mean(base_potentials):.2f}'})
                    
                    pbar.update(1)
            
            # Add realistic noise
            noise = np.random.normal(0, 0.5, len(corpus_df))  # Increased noise for realism
            final_potentials = np.array(base_potentials) + noise
            
            corpus_with_psi['electrostatic_potential'] = final_potentials
            
            logger.info(f"‚úÖ Generated {len(final_potentials)} placeholder potentials")
            logger.info(f"üìä Placeholder œà statistics: {np.mean(final_potentials):.2f}¬±{np.std(final_potentials):.2f} kT/e")
            logger.info(f"üìà Placeholder œà range: [{np.min(final_potentials):.2f}, {np.max(final_potentials):.2f}] kT/e")
        
        # Save results
        corpus_with_psi.to_csv(output_path, sep='\t', index=False)
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Electrostatics computation completed in {elapsed:.1f}s")
        
        return str(output_path)
    
    def step3_train_model(self, corpus_with_psi_path: str) -> Dict[str, Any]:
        """Step 3: Train TileFormer model."""
        logger.info("=" * 60)
        logger.info("üèãÔ∏è STEP 3: Training TileFormer model")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Load data
        corpus_df = pd.read_csv(corpus_with_psi_path, sep='\t')
        
        # Filter out sequences without potentials
        valid_mask = corpus_df['electrostatic_potential'].notna()
        corpus_df = corpus_df[valid_mask].reset_index(drop=True)
        
        logger.info(f"üìä Training data: {len(corpus_df):,} sequences")
        
        # Extract sequences and potentials
        sequences = corpus_df['sequence'].tolist()
        potentials = corpus_df['electrostatic_potential'].values.astype(np.float32)
        
        # Validate electrostatic potential values
        logger.info(f"üìä Electrostatic potential statistics:")
        logger.info(f"   Range: [{np.min(potentials):.3f}, {np.max(potentials):.3f}] kT/e")
        logger.info(f"   Mean¬±Std: {np.mean(potentials):.3f}¬±{np.std(potentials):.3f} kT/e")
        logger.info(f"   Valid values: {np.sum(np.isfinite(potentials))}/{len(potentials)}")
        
        # Check for extreme or invalid values
        extreme_mask = np.abs(potentials) > 1000
        if np.any(extreme_mask):
            logger.warning(f"‚ö†Ô∏è Found {np.sum(extreme_mask)} extreme œà values (|œà| > 1000 kT/e)")
        
        invalid_mask = ~np.isfinite(potentials)
        if np.any(invalid_mask):
            logger.error(f"‚ùå Found {np.sum(invalid_mask)} invalid œà values (NaN/Inf)")
            raise ValueError("Invalid electrostatic potential values detected!")
        
        # Data splits
        train_size = int(self.config['train_split'] * len(sequences))
        val_size = int(self.config['val_split'] * len(sequences))
        test_size = len(sequences) - train_size - val_size
        
        # Random split
        indices = np.random.permutation(len(sequences))
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        test_indices = indices[train_size + val_size:]
        
        # Create datasets
        train_dataset = DNADataset(
            [sequences[i] for i in train_indices],
            potentials[train_indices],
            use_reverse_complement=self.config['use_reverse_complement'],
            use_shift_augmentation=self.config['use_shift_augmentation'],
            max_shift=self.config['max_shift']
        )
        
        val_dataset = DNADataset(
            [sequences[i] for i in val_indices],
            potentials[val_indices],
            use_reverse_complement=False,  # No augmentation for validation
            use_shift_augmentation=False,
            max_shift=0
        )
        
        test_dataset = DNADataset(
            [sequences[i] for i in test_indices],
            potentials[test_indices],
            use_reverse_complement=False,  # No augmentation for test
            use_shift_augmentation=False,
            max_shift=0
        )
        
        # Create data loaders with GPU optimization
        num_workers = min(8, os.cpu_count())  # Optimize for available CPUs
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True,  # Keep workers alive between epochs
            prefetch_factor=2  # Prefetch batches for GPU
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['test_batch_size'],
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config['test_batch_size'],
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
        
        # Create model
        model_config = TileFormerConfig(
            stem_out_channels=self.config['stem_out_channels'],
            d_model=self.config['d_model'],
            n_heads=self.config['n_heads'],
            n_layers=self.config['n_layers'],
            ffn_dim=self.config['ffn_dim'],
            dropout=self.config['dropout'],
            mlp_hidden_dims=tuple(self.config['mlp_hidden_dims']),
            mlp_dropout=self.config['mlp_dropout'],
            learning_rate=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            batch_size=self.config['batch_size'],
            max_epochs=self.config['max_epochs']
        )
        
        self.model = TileFormer(
            stem_out_channels=model_config.stem_out_channels,
            d_model=model_config.d_model,
            n_heads=model_config.n_heads,
            n_layers=model_config.n_layers,
            ffn_dim=model_config.ffn_dim,
            dropout=model_config.dropout,
            mlp_hidden_dims=model_config.mlp_hidden_dims,
            mlp_dropout=model_config.mlp_dropout
        )
        
        self.model.to(self.device)
        
        # Count parameters
        total_params = count_parameters(self.model)
        logger.info(f"üìä Model parameters: {total_params:,}")
        
        # Model sanity check: test forward pass
        logger.info("üß™ Running model sanity check...")
        self.model.eval()
        with torch.no_grad():
            # Get a small test batch
            test_batch = next(iter(train_loader))
            test_x, test_y = test_batch[0][:4], test_batch[1][:4]  # Use first 4 samples
            test_x, test_y = test_x.to(self.device), test_y.to(self.device)
            
            # Test forward pass
            test_pred = self.model(test_x)
            
            logger.info(f"   Input shape: {test_x.shape}")
            logger.info(f"   Output shape: {test_pred.shape}")
            logger.info(f"   Target range: [{test_y.min():.3f}, {test_y.max():.3f}]")
            logger.info(f"   Initial pred range: [{test_pred.min():.3f}, {test_pred.max():.3f}]")
            
            if torch.isnan(test_pred).any() or torch.isinf(test_pred).any():
                raise RuntimeError("‚ùå Model produces NaN/Inf values in forward pass!")
            
            logger.info("‚úÖ Model sanity check passed")
        
        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # Enhanced training loop with comprehensive monitoring
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []
        
        # Training monitoring variables
        batch_losses = []
        gradient_norms = []
        prediction_stats = []
        
        logger.info("üöÄ Starting training with enhanced monitoring...")
        logger.info(f"üìä Batches per epoch: {len(train_loader)}")
        logger.info(f"üîç Progress logging every 50 batches")
        
        for epoch in range(self.config['max_epochs']):
            # Training phase
            self.model.train()
            epoch_train_loss = 0.0
            epoch_batch_losses = []
            epoch_grad_norms = []
            epoch_pred_stats = []
            
            # Progress bar for training batches
            train_pbar = tqdm(
                enumerate(train_loader), 
                total=len(train_loader),
                desc=f"üèãÔ∏è Epoch {epoch+1}/{self.config['max_epochs']} [Train]",
                leave=False
            )
            
            for batch_idx, (x, y) in train_pbar:
                x, y = x.to(self.device), y.to(self.device).unsqueeze(-1)
                
                # Forward pass
                optimizer.zero_grad()
                y_pred = self.model(x)
                loss = criterion(y_pred, y)
                
                # Backward pass
                loss.backward()
                
                # Gradient monitoring and validation
                total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['grad_clip'])
                
                # Check for zero gradients (indicates gradient flow issues)
                if batch_idx == 0:  # Check first batch of each epoch
                    zero_grad_params = 0
                    total_params = 0
                    for name, param in self.model.named_parameters():
                        if param.grad is not None:
                            zero_grad_count = (param.grad.abs() < 1e-8).sum().item()
                            total_params += param.numel()
                            zero_grad_params += zero_grad_count
                    
                    if total_params > 0:
                        zero_grad_ratio = zero_grad_params / total_params
                        if zero_grad_ratio > 0.9:
                            logger.warning(f"‚ö†Ô∏è {zero_grad_ratio:.1%} parameters have near-zero gradients")
                
                optimizer.step()
                
                # Loss and statistics tracking
                batch_loss = loss.item()
                epoch_train_loss += batch_loss
                epoch_batch_losses.append(batch_loss)
                epoch_grad_norms.append(total_norm.item() if isinstance(total_norm, torch.Tensor) else total_norm)
                
                # Prediction statistics
                with torch.no_grad():
                    pred_mean = y_pred.mean().item()
                    pred_std = y_pred.std().item()
                    target_mean = y.mean().item()
                    target_std = y.std().item()
                    
                    epoch_pred_stats.append({
                        'pred_mean': pred_mean,
                        'pred_std': pred_std,
                        'target_mean': target_mean,
                        'target_std': target_std,
                        'loss': batch_loss
                    })
                
                # Update progress bar
                train_pbar.set_postfix({
                    'Loss': f'{batch_loss:.4f}',
                    'Avg': f'{np.mean(epoch_batch_losses):.4f}',
                    'GradNorm': f'{epoch_grad_norms[-1]:.3f}'
                })
                
                # Calculate global batch number for sparse logging
                global_batch = epoch * len(train_loader) + batch_idx + 1
                
                # Very sparse detailed training logging (every 1000 batches)
                if global_batch % 1000 == 0:
                    with torch.no_grad():
                        # Tensor shape analysis
                        logger.info(f"üî¨ TRAINING-1K (Global batch {global_batch}):")
                        logger.info(f"   Input shape: {x.shape}, Target shape: {y.shape}")
                        logger.info(f"   Prediction shape: {y_pred.shape}")
                        logger.info(f"   Device: {x.device}, Dtype: {x.dtype}")
                        
                        # Forward pass analysis
                        logger.info(f"   Forward pass - Loss: {batch_loss:.6f}")
                        logger.info(f"   Predictions: min={y_pred.min().item():.4f}, max={y_pred.max().item():.4f}")
                        logger.info(f"   Targets: min={y.min().item():.4f}, max={y.max().item():.4f}")
                        
                        # Gradient analysis
                        grad_stats = []
                        param_count = 0
                        for name, param in self.model.named_parameters():
                            if param.grad is not None:
                                grad_norm = param.grad.norm().item()
                                grad_stats.append((name, grad_norm, param.numel()))
                                param_count += param.numel()
                        
                        if grad_stats:
                            grad_stats.sort(key=lambda x: x[1], reverse=True)  # Sort by gradient norm
                            logger.info(f"   Gradient analysis ({param_count:,} parameters):")
                            logger.info(f"   Top gradient norms: {grad_stats[0][0]}={grad_stats[0][1]:.6f}, "
                                       f"{grad_stats[1][0]}={grad_stats[1][1]:.6f}")
                            logger.info(f"   Bottom gradient norms: {grad_stats[-1][0]}={grad_stats[-1][1]:.6f}")
                        
                        # Memory usage (if CUDA)
                        if torch.cuda.is_available():
                            memory_used = torch.cuda.memory_allocated(self.device) / 1e9
                            memory_cached = torch.cuda.memory_reserved(self.device) / 1e9
                            logger.info(f"   GPU memory: {memory_used:.2f}GB used, {memory_cached:.2f}GB cached")
                
                # Regular detailed logging every 50 batches
                if (batch_idx + 1) % 50 == 0:
                    recent_losses = epoch_batch_losses[-50:]
                    recent_grads = epoch_grad_norms[-50:]
                    recent_stats = epoch_pred_stats[-50:]
                    
                    avg_loss = np.mean(recent_losses)
                    avg_grad = np.mean(recent_grads)
                    avg_pred_mean = np.mean([s['pred_mean'] for s in recent_stats])
                    avg_target_mean = np.mean([s['target_mean'] for s in recent_stats])
                    
                    logger.info(f"  üìà Batch {batch_idx+1:4d}: "
                               f"Loss={avg_loss:.4f}, "
                               f"GradNorm={avg_grad:.3f}, "
                               f"PredMean={avg_pred_mean:.3f}, "
                               f"TargetMean={avg_target_mean:.3f}")
                    
                    # Check for extreme values
                    if avg_loss > 100 or avg_grad > 10:
                        logger.warning(f"‚ö†Ô∏è Extreme values detected! Loss={avg_loss:.4f}, GradNorm={avg_grad:.3f}")
                    
                    if np.abs(avg_pred_mean) > 1000:
                        logger.warning(f"‚ö†Ô∏è Unusual prediction magnitude: {avg_pred_mean:.3f}")
                
                # Learning progress monitoring every 100 batches
                if (batch_idx + 1) % 100 == 0:
                    recent_100_losses = epoch_batch_losses[-100:] if len(epoch_batch_losses) >= 100 else epoch_batch_losses
                    if len(recent_100_losses) >= 10:
                        loss_trend = np.polyfit(range(len(recent_100_losses)), recent_100_losses, 1)[0]
                        logger.debug(f"   üìä Learning trend (last {len(recent_100_losses)} batches): "
                                   f"slope={loss_trend:.6f} {'üìâ' if loss_trend < 0 else 'üìà'}")
                
                # Error checking for NaN/Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    logger.error(f"‚ùå NaN/Inf loss detected at batch {batch_idx}!")
                    logger.error(f"  Predictions: min={y_pred.min().item():.3f}, max={y_pred.max().item():.3f}")
                    logger.error(f"  Targets: min={y.min().item():.3f}, max={y.max().item():.3f}")
                    raise ValueError("Training failed due to NaN/Inf loss")
            
            avg_train_loss = epoch_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            batch_losses.extend(epoch_batch_losses)
            gradient_norms.extend(epoch_grad_norms)
            prediction_stats.extend(epoch_pred_stats)
            
            # Validation phase with progress bar
            self.model.eval()
            epoch_val_loss = 0.0
            val_predictions = []
            val_targets = []
            
            val_pbar = tqdm(
                val_loader,
                desc=f"üß™ Epoch {epoch+1}/{self.config['max_epochs']} [Val]",
                leave=False
            )
            
            with torch.no_grad():
                for x, y in val_pbar:
                    x, y = x.to(self.device), y.to(self.device).unsqueeze(-1)
                    y_pred = self.model(x)
                    loss = criterion(y_pred, y)
                    epoch_val_loss += loss.item()
                    
                    val_predictions.extend(y_pred.cpu().numpy().flatten())
                    val_targets.extend(y.cpu().numpy().flatten())
                    
                    val_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
            
            avg_val_loss = epoch_val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            
            # Comprehensive epoch logging
            val_pred_mean = np.mean(val_predictions)
            val_target_mean = np.mean(val_targets)
            val_correlation = np.corrcoef(val_predictions, val_targets)[0, 1]
            
            logger.info(f"üìä Epoch {epoch + 1}/{self.config['max_epochs']} Summary:")
            logger.info(f"  üèãÔ∏è Train Loss: {avg_train_loss:.6f}")
            logger.info(f"  üß™ Val Loss:   {avg_val_loss:.6f}")
            logger.info(f"  üìà Val Corr:   {val_correlation:.4f}")
            logger.info(f"  üìè Avg GradNorm: {np.mean(epoch_grad_norms):.3f}")
            logger.info(f"  üéØ Pred Mean: {val_pred_mean:.3f}, Target Mean: {val_target_mean:.3f}")
            
            # Early stopping with enhanced logging
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                
                logger.info(f"  ‚≠ê New best validation loss! Saving model...")
                
                # Save best model with enhanced metadata
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'val_correlation': val_correlation,
                    'config': model_config.__dict__,
                    'training_stats': {
                        'batch_losses': batch_losses[-1000:],  # Keep last 1000 batches
                        'gradient_norms': gradient_norms[-1000:],
                        'prediction_stats': prediction_stats[-100:]  # Keep last 100 batches
                    }
                }, Path(self.config['output_dir']) / 'best_model.pt')
                
            else:
                patience_counter += 1
                logger.info(f"  ‚è≥ Patience: {patience_counter}/{self.config['early_stopping_patience']}")
                
                if patience_counter >= self.config['early_stopping_patience']:
                    logger.info(f"‚èπÔ∏è Early stopping at epoch {epoch + 1}")
                    break
        
        # Load best model for evaluation
        checkpoint = torch.load(Path(self.config['output_dir']) / 'best_model.pt')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Training completed in {elapsed:.1f}s")
        logger.info(f"üéØ Best validation loss: {best_val_loss:.6f}")
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_loss': best_val_loss,
            'total_epochs': epoch + 1,
            'test_loader': test_loader,
            'test_df': corpus_df.iloc[test_indices].reset_index(drop=True)
        }
    
    def step4_evaluate_model(self, training_results: Dict[str, Any]) -> Dict[str, Any]:
        """Step 4: Comprehensive model evaluation."""
        logger.info("=" * 60)
        logger.info("üìä STEP 4: Comprehensive model evaluation")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        test_loader = training_results['test_loader']
        test_df = training_results['test_df']
        
        # Generate predictions
        self.model.eval()
        y_true_list = []
        y_pred_list = []
        
        with torch.no_grad():
            for x, y in test_loader:
                x, y = x.to(self.device), y.to(self.device)
                y_pred = self.model(x).squeeze(-1)  # Model outputs (B,1) -> (B,)
                
                # Ensure both are same shape (B,) for evaluation
                y_true_list.append(y.cpu().numpy())  # Dataset outputs (B,)
                y_pred_list.append(y_pred.cpu().numpy())  # Now also (B,)
        
        y_true = np.concatenate(y_true_list)
        y_pred = np.concatenate(y_pred_list)
        
        # Validate evaluation data
        logger.info(f"üìä Evaluation data validation:")
        logger.info(f"   Test samples: {len(y_true)}")
        logger.info(f"   True range: [{np.min(y_true):.3f}, {np.max(y_true):.3f}] kT/e")
        logger.info(f"   Pred range: [{np.min(y_pred):.3f}, {np.max(y_pred):.3f}] kT/e")
        logger.info(f"   Shapes: y_true={y_true.shape}, y_pred={y_pred.shape}")
        
        # Check for evaluation issues
        if len(y_true) != len(y_pred):
            raise ValueError(f"Shape mismatch: y_true={y_true.shape} vs y_pred={y_pred.shape}")
        
        if np.any(~np.isfinite(y_true)) or np.any(~np.isfinite(y_pred)):
            n_invalid_true = np.sum(~np.isfinite(y_true))
            n_invalid_pred = np.sum(~np.isfinite(y_pred))
            logger.error(f"‚ùå Invalid values: {n_invalid_true} in targets, {n_invalid_pred} in predictions")
            raise ValueError("NaN/Inf values detected in evaluation data!")
        
        # Quick correlation check
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        logger.info(f"üìà Initial correlation: {correlation:.4f}")
        
        # Comprehensive evaluation
        eval_results = self.evaluator.evaluate_all(
            y_true=y_true,
            y_pred=y_pred,
            df=test_df,
            y_std=None  # No uncertainty estimates for now
        )
        
        # Save results
        results_path = Path(self.config['output_dir']) / 'evaluation_results.json'
        self.evaluator.save_results(eval_results, results_path)
        
        # Save predictions if requested
        if self.config['save_predictions']:
            pred_df = test_df.copy()
            pred_df['predicted_potential'] = y_pred
            pred_df['true_potential'] = y_true
            pred_df['residual'] = y_pred - y_true
            
            pred_path = Path(self.config['output_dir']) / 'test_predictions.tsv'
            pred_df.to_csv(pred_path, sep='\t', index=False)
            logger.info(f"üíæ Test predictions saved: {pred_path}")
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Evaluation completed in {elapsed:.1f}s")
        
        # Print summary
        summary = eval_results['summary']['overall_performance']
        logger.info("üìà EVALUATION SUMMARY:")
        logger.info(f"   RMSE: {summary['rmse']:.6f}")
        logger.info(f"   MAE:  {summary['mae']:.6f}")
        logger.info(f"   R¬≤:   {summary['r2']:.6f}")
        logger.info(f"   Pearson r: {summary['pearson_r']:.6f}")
        
        return eval_results
    
    def verify_pipeline_readiness(self) -> bool:
        """Verify that all pipeline components are ready and functional."""
        logger.info("üîç Verifying pipeline readiness...")
        
        try:
            # 1. Check ENCODE4 file accessibility
            encode4_path = self.config['encode4_path']
            if not os.path.exists(encode4_path):
                logger.error(f"‚ùå ENCODE4 file not found: {encode4_path}")
                return False
            
            file_size_gb = os.path.getsize(encode4_path) / (1024**3)
            logger.info(f"‚úÖ ENCODE4 file accessible: {file_size_gb:.2f} GB")
            
            # 2. Test model instantiation
            from models.tileformer_exact import TileFormer, TileFormerConfig
            test_config = TileFormerConfig(
                stem_out_channels=32,  # Smaller for testing
                d_model=96,
                n_heads=2,
                n_layers=1
            )
            test_model = TileFormer(
                stem_out_channels=test_config.stem_out_channels,
                d_model=test_config.d_model,
                n_heads=test_config.n_heads,
                n_layers=test_config.n_layers,
                ffn_dim=test_config.ffn_dim,
                dropout=test_config.dropout,
                mlp_hidden_dims=test_config.mlp_hidden_dims,
                mlp_dropout=test_config.mlp_dropout
            )
            test_model.to(self.device)
            logger.info("‚úÖ Model instantiation successful")
            
            # 3. Test forward pass
            test_input = torch.randn(2, 4, 20).to(self.device)  # Batch of 2
            with torch.no_grad():
                test_output = test_model(test_input)
            
            if test_output.shape != (2, 1):
                logger.error(f"‚ùå Model output shape incorrect: {test_output.shape}, expected (2, 1)")
                return False
            
            logger.info(f"‚úÖ Forward pass successful: {test_input.shape} -> {test_output.shape}")
            
            # 4. Test data pipeline
            test_sequences = ["ATCGATCGATCGATCGATCG", "GCTAGCTAGCTAGCTAGCTA"]
            test_potentials = np.array([-10.5, -15.2], dtype=np.float32)
            
            test_dataset = DNADataset(test_sequences, test_potentials)
            test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)
            
            test_batch = next(iter(test_loader))
            test_x, test_y = test_batch
            
            if test_x.shape != (2, 4, 20) or test_y.shape != (2,):
                logger.error(f"‚ùå Data pipeline shapes incorrect: x={test_x.shape}, y={test_y.shape}")
                return False
            
            logger.info(f"‚úÖ Data pipeline successful: x={test_x.shape}, y={test_y.shape}")
            
            # 5. Test loss computation
            test_x = test_x.to(self.device)
            test_y = test_y.to(self.device).unsqueeze(-1)
            
            with torch.no_grad():
                test_pred = test_model(test_x)
                test_loss = nn.MSELoss()(test_pred, test_y)
            
            if torch.isnan(test_loss) or torch.isinf(test_loss):
                logger.error(f"‚ùå Loss computation produces NaN/Inf: {test_loss}")
                return False
            
            logger.info(f"‚úÖ Loss computation successful: {test_loss.item():.6f}")
            
            # Clean up test objects
            del test_model, test_input, test_output, test_dataset, test_loader
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("üéâ Pipeline readiness verification completed successfully!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline verification failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_complete_pipeline(self) -> Dict[str, Any]:
        """Run the complete TileFormer training pipeline."""
        logger.info("üöÄ Starting complete TileFormer training pipeline...")
        
        # Create output directories with logging
        output_dir = Path(self.config['output_dir'])
        data_dir = Path(self.config['data_dir'])
        plots_dir = Path(self.config['plots_dir'])
        logs_dir = Path(self.config.get('logs_dir', 'logs'))
        
        output_dir.mkdir(parents=True, exist_ok=True)
        data_dir.mkdir(parents=True, exist_ok=True)
        plots_dir.mkdir(parents=True, exist_ok=True)
        logs_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üìÅ Directory structure created:")
        logger.info(f"   üìä Data: {data_dir}")
        logger.info(f"   üìà Outputs: {output_dir}")
        logger.info(f"   üìâ Plots: {plots_dir}")
        logger.info(f"   üìù Logs: {logs_dir}")
        
        # Verify pipeline readiness before starting
        if not self.verify_pipeline_readiness():
            raise RuntimeError("‚ùå Pipeline verification failed! Cannot proceed with training.")
        
        # Log monitoring information
        logger.info("üìù Log files created for monitoring:")
        logger.info(f"   üìã Main training log: {self.log_files[0]}")
        logger.info(f"   üîç Debug log: {self.log_files[1]}")
        logger.info(f"   ‚ö° ABPS processing log: {self.log_files[2]}")
        logger.info("üí° Use 'tail -f <logfile>' to monitor progress in real-time")
        
        overall_start = time.time()
        
        try:
            # Step 1: Generate corpus
            corpus_path = self.step1_generate_corpus()
            
            # Step 2: Compute electrostatics
            corpus_with_psi_path = self.step2_compute_electrostatics(corpus_path)
            
            # Step 3: Train model
            training_results = self.step3_train_model(corpus_with_psi_path)
            
            # Step 4: Evaluate model
            eval_results = self.step4_evaluate_model(training_results)
            
            # Final summary
            total_time = time.time() - overall_start
            
            final_results = {
                'pipeline_completed': True,
                'total_time_seconds': total_time,
                'training_results': training_results,
                'evaluation_results': eval_results,
                'config': self.config
            }
            
            logger.info("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"‚è±Ô∏è Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
            
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description='Complete TileFormer Training Pipeline')
    parser.add_argument('--config', type=str, default=None,
                       help='Path to configuration file (YAML)')
    parser.add_argument('--output-dir', type=str, default='outputs',
                       help='Output directory')
    parser.add_argument('--abps-method', type=str, default='placeholder',
                       choices=['tleap', 'placeholder'],
                       help='ABPS method to use')
    parser.add_argument('--batch-size', type=int, default=256,
                       help='Training batch size')
    parser.add_argument('--max-epochs', type=int, default=100,
                       help='Maximum training epochs')
    parser.add_argument('--gpu-id', type=int, default=1,
                       help='GPU ID to use (default: 1)')
    parser.add_argument('--corpus-size', type=int, default=50000,
                       help='Number of sequences to generate (default: 50000)')
    
    args = parser.parse_args()
    
    # Override config with command line arguments
    config_overrides = {
        'output_dir': args.output_dir,
        'abps_method': args.abps_method,
        'batch_size': args.batch_size,
        'max_epochs': args.max_epochs,
        'corpus_size': args.corpus_size
    }
    
    # Initialize trainer with specific GPU
    trainer = TileFormerTrainer(args.config, gpu_id=args.gpu_id)
    
    # Apply overrides
    trainer.config.update(config_overrides)
    
    # Run pipeline
    results = trainer.run_complete_pipeline()
    
    print("‚úÖ TileFormer training pipeline completed successfully!")
    return results

if __name__ == "__main__":
    main() 